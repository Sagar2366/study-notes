{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.fastai.learner import *\n",
    "from fastai.fastai.column_data import *\n",
    "from fastai.fastai.io import *\n",
    "from fastai.fastai.lm_rnn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00:00:00 - Part 1 recap\n",
    "\n",
    "* Part 1 theme = classification and regression with DL.\n",
    "  * Identify and learning best practises.\n",
    "* First 4 lessons: image classification, structured data and NLP in practise.\n",
    "* Last 3 lessons: understanding more detail about what is going on under the hood.\n",
    "\n",
    "## 00:01:01 - Part 2 preview\n",
    "\n",
    "* Move from classification focus to generative models:\n",
    "  * Chat responses.\n",
    "  * Images.\n",
    "  * Text.\n",
    "* Move from best practises to speculative stuff:\n",
    "  * Recent papers that haven't been fully tested.\n",
    "* Learn how to read papers.\n",
    "\n",
    "## 00:02:51 - RNNs (recap of Lesson 6)\n",
    "\n",
    "* RNNs are just standard fully-connected networks.\n",
    "* Recap of lesson from last week (see Lesson 6 notebook).\n",
    "\n",
    "### 00:06:20 - Multi-output model\n",
    "\n",
    "* Split into non-overlapping pieces, the use a piece to predict the next chars offset by 1.\n",
    "* Problem with RNN model created earlier: each time we start a new sequence, we have to learn the hidden state from scratch:\n",
    "  ```\n",
    "  def forward(self, *cs):\n",
    "      bs = cs[0].size(0)\n",
    "      \n",
    "      # Problem: we are created a brand new hidden state each forward prop\n",
    "      h = V(torch.zeros(1, bs, n_hidden)\n",
    "      \n",
    "      inp = self.e(torch.stack(cs))\n",
    "      outp, h = self.rnn(inp, h)\n",
    "  ```\n",
    "  * Can improve on that by saving the state of `self.h` in the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size[0]\n",
    "        \n",
    "        # This handles the last batch, if we don't have enough\n",
    "        # text for a batch size.\n",
    "        if self.h.size(1) != bs:\n",
    "            self.init_hidden(bs)\n",
    "            \n",
    "        outp, h = self.rnn(self.e(cs), self.h)\n",
    "        \n",
    "        # Store results of hidden layer and throw away history of operations.\n",
    "        # Called: backprop through time.\n",
    "        self.h = repackage_var(h)\n",
    "        \n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(1, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00:10:50 - Backprop through time\n",
    "\n",
    "* In multi-output model, unrolled RNN is going to be the size of the corpus. Eg if it's a million words, `self.h` would have a million layers, which would be expensive to run backprop etc.\n",
    "  * Want to remember state but not history: `Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)`\n",
    "    * By passing `h.data` into a new `Variable` class, you lose the history.\n",
    "* Process of running back prop through hidden state history is backprop through time.\n",
    "* Usually set a cap on how many layers to run bptt on.\n",
    "  * In original RNN lesson, we had a var called `bptt = 70`, which sets how many layers to run backprop through.\n",
    "* Longer values may let you capture more state about the problem, but may also results in exploding / vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00:16:00 - Minibatching in RNNs\n",
    "\n",
    "* How do you get the data into the model, given we want to do a mini-batch of sections?\n",
    "* Reminder about how Torch text breaks up documents for batching:\n",
    "  * Split the corpus into 64 equal sized chunks.\n",
    "  * Stack the \"chunks\" - each mini batch splits the chunks of size `bptt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00:20:31 - Augmentation for NLP (audience question)\n",
    "\n",
    "* No good way known: Jeremy planning to study it.\n",
    "* Recent Kaggle winner won it by randomly inserting parts of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00:21:38 - Choosing a `bptt` size (audience question)\n",
    "\n",
    "* Question: how do you choose a `bptt` size?\n",
    "* Answers:\n",
    "  * Memory: Matrix size for a minibatch has a bptt x bs size, so pick one that your GPU can handle.\n",
    "  * Stability: If training is unstable: loss shotting off to NaN can try reducing (less layers to gradient explode through).\n",
    "  * Performance: Consider reducing if each batch is training too slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00:23:24 - Training model with Torchtext tooling\n",
    "\n",
    "* Back to Torchtext for preparing dataset.\n",
    "* Have a choice when preparing dataset:\n",
    "  * Write your own dataset subclass\n",
    "  * Change your data to fit the dataset classes you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.fastai.nlp import *\n",
    "from fastai.fastai.lm_rnn import *\n",
    "\n",
    "PATH = 'data/nietzche/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset (not covered in lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepared data by copying nietzsche.txt data to `trn` and `val` folder, then deleting last 20% of rows in `trn` and deleted first 80% of rows in `val`.\n",
    "* Useful to not have a random shuffled validation set: val set is actual ordered text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {TRN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {VAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nietzsche.txt: 606kB [00:05, 113kB/s]                             \n"
     ]
    }
   ],
   "source": [
    "get_data('https://s3.amazonaws.com/text-datasets/nietzsche.txt', f'{PATH}nietzsche.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9934 data/nietzche/nietzsche.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {PATH}nietzsche.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ceil over round because that line ends a sentence.\n",
    "trn_rows = math.ceil(9934 * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n {trn_rows} {PATH}nietzsche.txt > {TRN}/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that the easy life which the Jesuit manuals advocate is for the benefit,\r\n",
      "not of the Jesuits but the laity. Indeed, it may be questioned whether\r\n",
      "we enlightened ones would become equally competent workers as the result\r\n",
      "of similar tactics and organization, and equally worthy of admiration as\r\n",
      "the result of self mastery, indefatigable industry and devotion.\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 5 {TRN}/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n {9934 - trn_rows} {PATH}nietzsche.txt > {VAL}/val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "56\r\n",
      "\r\n",
      "=Victory of Knowledge over Radical Evil.=--It proves a material gain to\r\n",
      "him who would attain knowledge to have had during a considerable period\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 {VAL}/val.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Torchtext, you create a `Field`.\n",
    "  * Description for how to process text.\n",
    "  * `tokenize` - since you want a character model, can just use the list function in Python to tokenise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y', 'o', ' ', 'w', 'a', 's', 's', 'u', 'p']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('yo wassup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Settings:\n",
    "  * `bs` - batch size (same setting as last notebook).\n",
    "  * `bptt` - size of backprop through time.\n",
    "  * `n_fac` - size of embedding.\n",
    "  * `n_hidden` - size of hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64; bptt=8; n_fac=42; n_hidden=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Fast.ai dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Length of data loader is how many batches to go through (should be equal to token num / batch size / bptt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943.3046875"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_ds[0].text) / bs / bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Not exactly that in Torchtext: bptt randomised a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 482972)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `TEXT` also contains an extra attribute called `vocab` containing:\n",
    "  * list of all unique items in vocab (`TEXT.vocab.itos`).\n",
    "  * reverse mapping from each item to number (`TEXT.vocab.stoi`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " ' ',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 'a',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " 'r',\n",
       " 'h',\n",
       " 'l',\n",
       " 'd',\n",
       " 'c',\n",
       " 'u',\n",
       " 'f',\n",
       " 'm',\n",
       " 'p',\n",
       " 'g',\n",
       " ',',\n",
       " 'y',\n",
       " 'w',\n",
       " 'b',\n",
       " 'v',\n",
       " '-',\n",
       " '.',\n",
       " '\"',\n",
       " 'k',\n",
       " 'x',\n",
       " ';',\n",
       " ':',\n",
       " 'q',\n",
       " 'j',\n",
       " '!',\n",
       " '?',\n",
       " '(',\n",
       " ')',\n",
       " \"'\",\n",
       " 'z',\n",
       " '1',\n",
       " '2',\n",
       " '=',\n",
       " '_',\n",
       " '3',\n",
       " '[',\n",
       " ']',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '8',\n",
       " '7',\n",
       " '9',\n",
       " '0',\n",
       " 'ä']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index()>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             ' ': 2,\n",
       "             'e': 3,\n",
       "             't': 4,\n",
       "             'i': 5,\n",
       "             'a': 6,\n",
       "             'o': 7,\n",
       "             'n': 8,\n",
       "             's': 9,\n",
       "             'r': 10,\n",
       "             'h': 11,\n",
       "             'l': 12,\n",
       "             'd': 13,\n",
       "             'c': 14,\n",
       "             'u': 15,\n",
       "             'f': 16,\n",
       "             'm': 17,\n",
       "             'p': 18,\n",
       "             'g': 19,\n",
       "             ',': 20,\n",
       "             'y': 21,\n",
       "             'w': 22,\n",
       "             'b': 23,\n",
       "             'v': 24,\n",
       "             '-': 25,\n",
       "             '.': 26,\n",
       "             '\"': 27,\n",
       "             'k': 28,\n",
       "             'x': 29,\n",
       "             ';': 30,\n",
       "             ':': 31,\n",
       "             'q': 32,\n",
       "             'j': 33,\n",
       "             '!': 34,\n",
       "             '?': 35,\n",
       "             '(': 36,\n",
       "             ')': 37,\n",
       "             \"'\": 38,\n",
       "             'z': 39,\n",
       "             '1': 40,\n",
       "             '2': 41,\n",
       "             '=': 42,\n",
       "             '_': 43,\n",
       "             '3': 44,\n",
       "             '[': 45,\n",
       "             ']': 46,\n",
       "             '4': 47,\n",
       "             '5': 48,\n",
       "             '6': 49,\n",
       "             '8': 50,\n",
       "             '7': 51,\n",
       "             '9': 52,\n",
       "             '0': 53,\n",
       "             'ä': 54,\n",
       "             'æ': 0,\n",
       "             'ë': 0,\n",
       "             'é': 0,\n",
       "             '<eos>': 0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        \n",
    "        # Check for final batch (might not equal batch size)\n",
    "        if self.h.size(1) != bs:\n",
    "            self.init_hidden(bs)\n",
    "\n",
    "        outp, h = self.rnn(self.e(cs), self.h)\n",
    "        \n",
    "        # Get rid of history\n",
    "        self.h = repackage_var(h)\n",
    "        \n",
    "        # Have to use `.view` to make output rank 2 tensor which is what's required by loss func.\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn(md.nt, n_fac, 512)\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Output uses `.view` because loss function can't handle rank 3 tensors: has to be rank 2 (or rank 4).\n",
    "* `dim` arg is required in `log_softmax` for Pytorch >= 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d824141c5764695aca8cceef608bcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.880812   1.850538  \n",
      "    1      1.702586   1.708077                              \n",
      "    2      1.628063   1.638349                              \n",
      "    3      1.570418   1.602545                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6025455]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7dd30505e946fa98b93d37e90fb677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.493271   1.558022  \n",
      "    1      1.494684   1.553495                              \n",
      "    2      1.493436   1.55244                               \n",
      "    3      1.48739    1.543987                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5439872]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00:42:49 - RNN unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roughly equivalent to what PyTorch is doing\n",
    "\n",
    "def RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn2(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNNCell(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs:\n",
    "            self.init_hidden(bs)\n",
    "            \n",
    "        o = self.h\n",
    "        \n",
    "        outp = []\n",
    "        for c in cs:\n",
    "            o = self.rnn(self.e(c), o)\n",
    "            outp.append(o)\n",
    "        outp = self.l_out(torch.stack(outp))\n",
    "        \n",
    "        self.h = repackage_var(o)\n",
    "        return F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn2(md.nt, n_fac, 512)\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00:44:05 - Why use Tanh (audience question)\n",
    "\n",
    "* Shape is similar to sigmoid but goes between -1 and 1.\n",
    "* Force it into a range to help avoid a gradient explosion, which may happen with a `relu`.\n",
    "\n",
    "\n",
    "## 00:46:43 - GRUCell\n",
    "\n",
    "* `RNNCell` is not commonly used in practise - even with `tanh` activation function, it tends to have trouble with gradient explosion.\n",
    "* Instead: use `GRUCell`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.wildml.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.36.51-AM.png\" width=400px>\n",
    "\n",
    "GRU Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contains a neural net in a neural net that learns how much to remember of hidden state:\n",
    "  * If you see a full stop, throw away hidden state (for example).\n",
    "* `r` (reset gate):\n",
    "  * input normally gets multiplied by weight matrix to create new activations.\n",
    "  * With reset gate, input doesn't get added to previous activation directly.\n",
    "  * Previous activations are first multiplied by the reset gate (between 0 and 1) before being added to input + hidden concat.\n",
    "\n",
    "  * Equation for reset: $r_t = \\sigma(W_r\\dot[h_t-1,x_t])$\n",
    "  \n",
    "  * Equal to matrix product of a weight matrix and concat of hidden state and new input.\n",
    "  * Basically a one layer neural net / logistic regression model: can be thought of a neural network within a neural network.\n",
    "  \n",
    "* `z` (update gate)\n",
    "  * decides to what degree do you use your new hidden state vs leaving it how it was.\n",
    "  * In other words, how important is it to remember the current input?\n",
    "  * Equation for update: $z_t = \\sigma(W_z\\dot[h_t-1,x_t])$\n",
    "  \n",
    "* Final update equation is a linear interpolation using $z_t$:\n",
    "\n",
    "  $$\n",
    "  \\hat{h}_t = \\text{tanh}(W\\cdot[r_t * h_t - 1, x_t]) \\\\\n",
    "  h_t = (1 - z_t) * h_t - 1 + z_t * \\hat{h}_t\n",
    "  $$\n",
    "  \n",
    "* Definition from PyTorch source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roughly equivalent to what PyTorch is doing\n",
    "\n",
    "def GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    gi = F.linear(input, w_ih, b_ih)\n",
    "    gh = F.linear(hidden, w_hh, b_hh)\n",
    "    i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "    h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "    \n",
    "    resetgate = F.sigmoid(i_r + h_r)\n",
    "    inputgate = F.sigmoid(i_i + h_i)\n",
    "    newgate = F.tanh(i_n + resetgate * h_n)\n",
    "    return newgate + inputgate * (hidden - newgate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00:52:36 - Replace RNNCell with GRUCell in char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
