{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.fastai.learner import *\n",
    "from fastai.fastai.column_data import *\n",
    "from fastai.fastai.io import *\n",
    "from fastai.fastai.lm_rnn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00:00:00 - Part 1 recap\n",
    "\n",
    "* Part 1 theme = classification and regression with DL.\n",
    "  * Identify and learning best practises.\n",
    "* First 4 lessons: image classification, structured data and NLP in practise.\n",
    "* Last 3 lessons: understanding more detail about what is going on under the hood.\n",
    "\n",
    "## 00:01:01 - Part 2 preview\n",
    "\n",
    "* Move from classification focus to generative models:\n",
    "  * Chat responses.\n",
    "  * Images.\n",
    "  * Text.\n",
    "* Move from best practises to speculative stuff:\n",
    "  * Recent papers that haven't been fully tested.\n",
    "* Learn how to read papers.\n",
    "\n",
    "## 00:02:51 - RNNs (recap of Lesson 6)\n",
    "\n",
    "* RNNs are just standard fully-connected networks.\n",
    "* Recap of lesson from last week (see Lesson 6 notebook).\n",
    "\n",
    "### 00:06:20 - Multi-output model\n",
    "\n",
    "* Split into non-overlapping pieces, the use a piece to predict the next chars offset by 1.\n",
    "* Problem with RNN model created earlier: each time we start a new sequence, we have to learn the hidden state from scratch:\n",
    "  ```\n",
    "  def forward(self, *cs):\n",
    "      bs = cs[0].size(0)\n",
    "      \n",
    "      # Problem: we are created a brand new hidden state each forward prop\n",
    "      h = V(torch.zeros(1, bs, n_hidden)\n",
    "      \n",
    "      inp = self.e(torch.stack(cs))\n",
    "      outp, h = self.rnn(inp, h)\n",
    "  ```\n",
    "  * Can improve on that by saving the state of `self.h` in the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size[0]\n",
    "        \n",
    "        # This handles the last batch, if we don't have enough\n",
    "        # text for a batch size.\n",
    "        if self.h.size(1) != bs:\n",
    "            self.init_hidden(bs)\n",
    "            \n",
    "        outp, h = self.rnn(self.e(cs), self.h)\n",
    "        \n",
    "        # Store results of hidden layer and throw away history of operations.\n",
    "        # Called: backprop through time.\n",
    "        self.h = repackage_var(h)\n",
    "        \n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(1, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00:10:50 - Backprop through time\n",
    "\n",
    "* In multi-output model, unrolled RNN is going to be the size of the corpus. Eg if it's a million words, `self.h` would have a million layers, which would be expensive to run backprop etc.\n",
    "  * Want to remember state but not history: `Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)`\n",
    "    * By passing `h.data` into a new `Variable` class, you lose the history.\n",
    "* Process of running back prop through hidden state history is backprop through time.\n",
    "* Usually set a cap on how many layers to run bptt on.\n",
    "  * In original RNN lesson, we had a var called `bptt = 70`, which sets how many layers to run backprop through.\n",
    "* Longer values may let you capture more state about the problem, but may also results in exploding / vanishing gradients.\n",
    "  \n",
    "### 00:16:00 - ??\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
