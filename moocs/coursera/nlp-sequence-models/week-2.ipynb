{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word representation\n",
    "\n",
    "* Problems with the standard 1-hot presentation of words: model finds it hard to learn relationships between words (king is to queen as man is to woman):\n",
    "  * Inner product between any 2 one-hot vectors is 0.\n",
    "  * Euclidean distance between any pair of vectors is the same.\n",
    "* Would be better to learn a set of features for each words that help to categorise it (eg gender, royal-ness, age, fruit-ness etc).\n",
    "* Embedding basically learns relevant features (usually not easy to interpret), generating a 300 (or whatever) dimensions vector for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings\n",
    "\n",
    "* Named entity recogition example: what happens if you come across a set of words that your model hasn't learning in the standard RNN approach? Enter embedding.\n",
    "* Embedding can be trained on much larger corpuses of text (1B - 100B words is not uncommon).\n",
    "* Embedding allows for transfer learning: models using pretrained embeddings can be trained on smaller corpuses (Ã  la ImageNet models in the image classificatino word).\n",
    "  * Embeddings can be finetuned with new data (if dataset if big enough).\n",
    "* Embeddings have been useful for: entity recognition, text summarisation, co-reference resolution, parsing.\n",
    "* Less useful for: language modeling, machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of word embeddings\n",
    "\n",
    "* Word embeddings can help with \"analogy reasoning\".\n",
    "  * Answering the question: \"Man is to woman as king is to ___ ?\"\n",
    "* With a 4 dimension embedding for man and woman, with a feature for gender, royalness, age and foodness, you could subtract them from each other:\n",
    "\n",
    "    ```\n",
    "    e_man - e_woman = [-2, 0, 0, 0]\n",
    "    ```\n",
    "    \n",
    "  If you did the same from king and queen, you may end up with a similar result:\n",
    "   \n",
    "    ```\n",
    "    e_king - e_queen = [-2, 0, 0, 0]\n",
    "    ```\n",
    "   \n",
    "  * What it captures is that the main different between the two word sets is the gender.\n",
    "  * Ideas first published in paper [Linguistic Regularities in Continuous Space Word Representations](https://www.aclweb.org/anthology/N13-1090).\n",
    "* Formally, you'd aim to find the $e_{w} that satisfies that expression:\n",
    "   \n",
    "   $e_{man} - e_{woman} \\approx e_{king} - e_{w}$\n",
    "   \n",
    "   * Find word $e_{w}$ that maximises this the similarity expression (ie has the highest degree of similiarity): $\\text{sim}(e_{w}, e_{king} - e_{man} + e_{woman})$\n",
    "   \n",
    "* Similarity function: cosine similarity:\n",
    "  * $sim(u, v) = \\frac{u^{t}v}{||u||_2 ||v||_1}$\n",
    "    * Without the denominator, it's basically the inner product: if they are similar, product will be large.\n",
    "    * With the denominator, normalises result between -1 and 1. Very distant = -1, very similar = 1.\n",
    "  * Could also use squared distance: $||u-v||^2$ (since it's a measure of dissimilarity, you should take the negative to maximise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding matrix will be a `(300, 10000)` ($E$) dimension matrix (if you have a 10k vocab and 300 latent factors).\n",
    "* You could look up a word value in the matrix using a `(10000, 1)` ($o_j$) one-hot encoded matrix and doing a dot product against the embedding matrix to get the 300 latent factors for the word:\n",
    "  $E * o_j = e_j$\n",
    "* In practise you'd generally just lookup the embedding value of a word using a dictionary lookup, but it's easier to represent mathematically as a dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning word embeddings\n",
    "\n",
    "* In the early days of learning word embeddings, the algorithms were quite complicated but over time, researchers discovered simpler ways to do it.\n",
    "* If you started with a sentence and took away the last word:\n",
    "  \n",
    "    \"I     want   a    glass   of     orange   ___.\"\n",
    "     4343  9665   1    3852    6161   6257    \n",
    "     \n",
    "    * You would then lookup the embedding for each word:\n",
    "     * \"I\"       $o_{4343} \\longrightarrow E \\longrightarrow e_{4343}$\n",
    "     * \"want\"    $o_{9665} \\longrightarrow E \\longrightarrow e_{9665}$\n",
    "     * \"a\"       $o_{1}    \\longrightarrow E \\longrightarrow e_{1}$\n",
    "     * \"glass\"   $o_{3852} \\longrightarrow E \\longrightarrow e_{3852}$\n",
    "     * \"of\"      $o_{3852} \\longrightarrow E \\longrightarrow e_{3852}$\n",
    "     * \"orange\"  $o_{6257} \\longrightarrow E \\longrightarrow e_{6257}$\n",
    "    * Then pass all the values into a neural network, which feeds into a softmax layer. Softmax classifies against the 10k values and outputs a single word.\n",
    "    * The number of weights in the final layer is dependant on the size of the \"fixed historic window\" hyperparam. In other words, you could choose to take the last n words to predict the output.\n",
    "    * The number of weights would be 4 times the number of latent factors, in the above example: `4 * 300`.\n",
    "    * Use backprop to find the ideal word embeddings for the task.\n",
    "    \n",
    "* More complex algorithms:\n",
    "  * Given a sentence: \"I want a glass of orange juice to go along with my cereal.\" try to pick one of the middle words.\n",
    "    * You might feed into the neural net 4 words on the left & right.\n",
    "    * You could also just pass in a single last word.\n",
    "    * You could also just take a nearby 1 word (glass, cereal etc) - this is called a skip gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "* \"A simpler and computational more effective way to learn embeddings\".\n",
    "* Given sentence: \"I want a glass of orange juice to go along with my cereal\", you find a context -> target pair randomly (called a 'skip gram'), ie `orange` and `juice`, or `orange` and `glass` or `orange` and `my`.\n",
    "* Probably won't do well on this problem, but you aren't looking for success on this problem, you want to learn a good embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
