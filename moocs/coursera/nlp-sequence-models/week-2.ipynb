{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word representation\n",
    "\n",
    "* Problems with the standard 1-hot presentation of words: model finds it hard to learn relationships between words (king is to queen as man is to woman):\n",
    "  * Inner product between any 2 one-hot vectors is 0.\n",
    "  * Euclidean distance between any pair of vectors is the same.\n",
    "* Would be better to learn a set of features for each words that help to categorise it (eg gender, royal-ness, age, fruit-ness etc).\n",
    "* Embedding basically learns relevant features (usually not easy to interpret), generating a 300 (or whatever) dimensions vector for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings\n",
    "\n",
    "* Named entity recogition example: what happens if you come across a set of words that your model hasn't learning in the standard RNN approach? Enter embedding.\n",
    "* Embedding can be trained on much larger corpuses of text (1B - 100B words is not uncommon).\n",
    "* Embedding allows for transfer learning: models using pretrained embeddings can be trained on smaller corpuses (Ã  la ImageNet models in the image classificatino word).\n",
    "  * Embeddings can be finetuned with new data (if dataset if big enough).\n",
    "* Embeddings have been useful for: entity recognition, text summarisation, co-reference resolution, parsing.\n",
    "* Less useful for: language modeling, machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of word embeddings\n",
    "\n",
    "* Word embeddings can help with \"analogy reasoning\".\n",
    "  * Answering the question: \"Man is to woman as king is to ___ ?\"\n",
    "* With a 4 dimension embedding for man and woman, with a feature for gender, royalness, age and foodness, you could subtract them from each other:\n",
    "\n",
    "    ```\n",
    "    e_man - e_woman = [-2, 0, 0, 0]\n",
    "    ```\n",
    "    \n",
    "  If you did the same from king and queen, you may end up with a similar result:\n",
    "   \n",
    "    ```\n",
    "    e_king - e_queen = [-2, 0, 0, 0]\n",
    "    ```\n",
    "   \n",
    "  * What it captures is that the main different between the two word sets is the gender.\n",
    "  * Ideas first published in paper [Linguistic Regularities in Continuous Space Word Representations](https://www.aclweb.org/anthology/N13-1090).\n",
    "* Formally, you'd aim to find the $e_{w} that satisfies that expression:\n",
    "   \n",
    "   $e_{man} - e_{woman} \\approx e_{king} - e_{w}$\n",
    "   \n",
    "   * Find word $e_{w}$ that maximises this the similarity expression (ie has the highest degree of similiarity): $\\text{sim}(e_{w}, e_{king} - e_{man} + e_{woman})$\n",
    "   \n",
    "* Similarity function: cosine similarity:\n",
    "  * $sim(u, v) = \\frac{u^{t}v}{||u||_2 ||v||_1}$\n",
    "    * Without the denominator, it's basically the inner product: if they are similar, product will be large.\n",
    "    * With the denominator, normalises result between -1 and 1. Very distant = -1, very similar = 1.\n",
    "  * Could also use squared distance: $||u-v||^2$ (since it's a measure of dissimilarity, you should take the negative to maximise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
